{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running the Matrix Factorization model, we make use of pyspark environment which has a built-in ALS function. All the other methods are user defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_2=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = subset_data_2.user_id.unique()\n",
    "user_dict = dict(zip(user_ids, range(len(user_ids))))\n",
    "subset_data_2['user_id_int']=subset_data_2.user_id.map(user_dict)\n",
    "\n",
    "business_ids = subset_data_2.business_id.unique()\n",
    "business_dict = dict(zip(business_ids, range(len(business_ids))))\n",
    "subset_data_2['business_id_int']=subset_data_2.business_id.map(business_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset created\n",
      "train dataset created\n"
     ]
    }
   ],
   "source": [
    "test_dataset = subset_data_2[subset_data_2.groupby('user_id')['date'].transform('max') == subset_data_2['date']]\n",
    "print(\"test dataset created\")\n",
    "train_dataset = pd.concat([subset_data_2, test_dataset]).drop_duplicates(keep=False)\n",
    "print(\"train dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978637414297792"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_1=pd.DataFrame(pd.Series(subset_data_2.user_id).value_counts().reset_index(drop=True))\n",
    "sparsity=((subset_data_2.user_id.nunique()*subset_data_2.business_id.nunique())-sum(sparse_1.user_id))/(subset_data_2.user_id.nunique()*subset_data_2.business_id.nunique())\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation setup\n",
    "def cross_validation_setup(ratings,n1):\n",
    "    (training,tune) = ratings.randomSplit([n1,1-n1],seed = 42)\n",
    "    return(training,tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross_validation_setup method is used for cross validation. This function divides the ratings dataframe into training, tune and test set. We will use the training set to train the model and tune set to tune the hyperparameters. Finally, with the best parameters, we will evaluate our model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Evaluation metric setup\n",
    "def accuracy(predictions,metric):\n",
    "    evaluator = RegressionEvaluator(metricName=metric, labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "    return(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy method is used for evaluating the model by passing the predictions dataframe with the actual and predicted columns along with the desired metric (eg, rmse or mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.createDataFrame(train_dataset[['user_id_int','business_id_int','rating']])\n",
    "(training,tune) = cross_validation_setup(ratings,0.8)\n",
    "test=spark.createDataFrame(test_dataset[['user_id_int','business_id_int','rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for training data = 0.5789969309878112\n",
      "Mean-Absolute error for training data = 0.40622480648294007\n"
     ]
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.05, userCol=\"user_id_int\", itemCol=\"business_id_int\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\",rank=10)\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the training data\n",
    "predictions_training = model.transform(training)\n",
    "rmse = accuracy(predictions_training,\"rmse\")\n",
    "print(\"Root-mean-square error for training data = \" + str(rmse))\n",
    "mae = accuracy(predictions_training,\"mae\")\n",
    "print(\"Mean-Absolute error for training data = \" + str(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we check if pyspark ALS is working fine by passing arbitrary parameters to the ALS function and fitting the model on the training dataset. Since we get an rmse and mae value, we are good to go ahead and tune our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 1.2385\n",
      "MAE:  1.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0020554974924238"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline model using surprise BaseLine\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import BaselineOnly\n",
    "from surprise import accuracy\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "training_new=training.toPandas()\n",
    "test_new=test.toPandas()\n",
    "\n",
    "algo = BaselineOnly()\n",
    "reader = Reader()\n",
    "data_train = Dataset.load_from_df(training_new, reader)\n",
    "data_test = Dataset.load_from_df(test_new, reader)\n",
    "\n",
    "trainset = data_train.build_full_trainset()\n",
    "algo.fit(trainset)\n",
    "\n",
    "\n",
    "\n",
    "trainset, testset = train_test_split(data_test, test_size=1.0)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to check if our recommendation algorithm is performing well is to compare it against a baseline model. A baseline model is one in which we don't use any predictions. It can be assumed as a lazy model to give ratings to the missing user-item pairs. The first baseline model we considered is the user-item bias which is given by the ALS model. The above code computes the baseline metric for the tune set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 1.2503016998650578\n",
      "Baseline MAE: 1.0070540501592762\n"
     ]
    }
   ],
   "source": [
    "training_df=training.toPandas()\n",
    "test_df=test.toPandas()\n",
    "temp_df=pd.DataFrame(training_df.groupby('user_id_int')['rating'].mean()).reset_index()\n",
    "temp_df=temp_df.rename(columns={\"rating\": \"avg_rating_user\"})\n",
    "temp_df_2=pd.DataFrame(training_df.groupby('business_id_int')['rating'].mean()).reset_index()\n",
    "temp_df_2=temp_df_2.rename(columns={\"rating\": \"avg_rating_business\"})\n",
    "test_df=test_df.merge(temp_df,how='inner',on='user_id_int')\n",
    "test_df=test_df.merge(temp_df_2,how='inner',on='business_id_int')\n",
    "test_df['prediction']=(test_df['avg_rating_user']+test_df['avg_rating_business'])/2\n",
    "baseline_avg_rmse=math.sqrt(mean_squared_error(test_df['rating'],test_df['prediction']))\n",
    "baseline_avg_mae=math.sqrt(mean_absolute_error(test_df['rating'],test_df['prediction']))\n",
    "print('Baseline RMSE:',baseline_avg_rmse)\n",
    "print('Baseline MAE:',baseline_avg_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also considered creating another baseline model which gives a missing user-item pair, a rating which is calculated using the average of 2 things:\n",
    "1) Average rating that particular user has given\n",
    "2) Average rating that particular movie has received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter tuning\n",
    "def tune_ALS(train_data, validation_data, maxIter, regParams, ranks):\n",
    "    start_time = time.time()\n",
    "    min_error = float('inf')\n",
    "    min_mae= float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = regParams[0]\n",
    "    best_model = None\n",
    "    l1=[]\n",
    "    l2=[]\n",
    "    l3=[]\n",
    "    l4=[]\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "                    # get ALS model\n",
    "            als = ALS(userCol=\"user_id_int\", itemCol=\"business_id_int\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\").setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
    "            # train ALS model\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            predictions = model.transform(validation_data)\n",
    "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            evaluator2 = RegressionEvaluator(metricName=\"mae\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            mae = evaluator2.evaluate(predictions)            \n",
    "            l1.append(rmse)\n",
    "            l2.append(mae)\n",
    "            if (rmse < min_error): #and (mae < min_mae):\n",
    "                min_error = rmse\n",
    "                min_mae= mae\n",
    "                best_rank = rank\n",
    "                best_model = model\n",
    "            l3.append(rank)\n",
    "            l4.append(reg)\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {}'.format(best_rank, best_regularization))\n",
    "    return (l1,l2,l3,l4,time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method is used for training the hyperparameters for our ALS model. It takes in the training data, tune data and a set of range of hyperparameter values to return the best model, i.e. one having the least rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best model has 10 latent factors and regularization = 0.01\n"
     ]
    }
   ],
   "source": [
    "param_tune=tune_ALS(training,tune,5,[0.01,0.05,0.1],[10,20,30,40,50,60,70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rank=param_tune[2][param_tune[0].index(min(param_tune[0]))]\n",
    "best_reg=param_tune[3][param_tune[0].index(min(param_tune[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying the range of hyperparameters for reularization and latent factors, we plot the model performance (rmse/mae) with respect to these parameters. Since there are two hyperparameters, we fix the regularization parameter and plot for different values of latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for test data = 1.6460970389877598\n",
      "Mean-Absolute error for test data = 1.3834931867984956\n"
     ]
    }
   ],
   "source": [
    "#Evaluation metric setup\n",
    "def accuracy(predictions,metric):\n",
    "    evaluator = RegressionEvaluator(metricName=metric, labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "    return(evaluator.evaluate(predictions))\n",
    "\n",
    "# Build the recommendation model using ALS on the test data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.1, userCol=\"user_id_int\", itemCol=\"business_id_int\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\",rank=70)\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions_test = model.transform(test)\n",
    "predictions_test_pandas=predictions_test.toPandas()\n",
    "predictions_test_pandas['prediction']=np.where(predictions_test_pandas['prediction']>5,5,predictions_test_pandas['prediction'])\n",
    "predictions_test=spark.createDataFrame(predictions_test_pandas)\n",
    "rmse = accuracy(predictions_test,\"rmse\")\n",
    "print(\"Root-mean-square error for test data = \" + str(rmse))\n",
    "mae = accuracy(predictions_test,\"mae\")\n",
    "print(\"Mean-Absolute error for test data = \" + str(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the best hyperparameter values from grid search, we use it to finally train the model on our training set and finally validate our model performance on the test set to report the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our model, we'll now use it to recommend movies to the users. The following codes will lead to the recommendation of top k movies to each user in our dataset. Using the recommendForUserSubset Function we recommend the movies which a particular user has not already watched. We use this function to recommend for training and test set separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRecs_training = model.recommendForUserSubset(training,100)\n",
    "recommend_movies_1_training=userRecs_training.toPandas()\n",
    "userRecs_test = model.recommendForUserSubset(test,100)\n",
    "recommend_movies_1_test=userRecs_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving top k recommendation for each user \n",
    "def top_k_recommendation(recommend_movies_1,subset_data_2):\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    for i in range(0,recommend_movies_1.shape[0]):\n",
    "        list3=[]\n",
    "        list4=[]\n",
    "        for j in range(0,100):\n",
    "            if(len(list3)==10):\n",
    "                break\n",
    "            user_i_ratings = list(subset_data_2.loc[subset_data_2['user_id_int'] == recommend_movies_1['user_id_int'][i]]\n",
    "                                  [['business_id_int']]['business_id_int'])\n",
    "            if (recommend_movies_1['recommendations'][i][j][0] in user_i_ratings):\n",
    "                continue\n",
    "            else:\n",
    "                list3.append(recommend_movies_1['recommendations'][i][j][0])\n",
    "                list4.append(recommend_movies_1['recommendations'][i][j][1])\n",
    "        list1.append(list3)\n",
    "        list2.append(list4)\n",
    "    return(list1,list2)\n",
    "recommend_training=top_k_recommendation(recommend_movies_1_training,subset_data_2)\n",
    "recommend_test=top_k_recommendation(recommend_movies_1_test,subset_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_movies_2_train=recommend_movies_1_training[['user_id_int']]\n",
    "recommend_movies_2_train['recommended_movies']=recommend_training[0]\n",
    "recommend_movies_2_train['predicted_ratings']=recommend_training[1]\n",
    "\n",
    "recommend_movies_2_test=recommend_movies_1_test[['user_id_int']]\n",
    "recommend_movies_2_test['recommended_movies']=recommend_test[0]\n",
    "recommend_movies_2_test['predicted_ratings']=recommend_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user_coverage method calculates the user coverage from our model. This is the proportion of users for which atleast k movies can be recommended well. First, we define what a good recommendation is and fix a value of k to calculate the coverage value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User coverage on training set: 0.852929292929293\n",
      "User coverage on test set: 0.852929292929293\n"
     ]
    }
   ],
   "source": [
    "#User Coverage\n",
    "def user_coverage(data,k,threshold):\n",
    "    sum=0\n",
    "    for i in range(0,data.shape[0]):\n",
    "        if((np.array(data['predicted_ratings'][i])>threshold).sum()>k):\n",
    "            sum+=1\n",
    "    user_coverage=sum/subset_data_2['user_id_int'].nunique()\n",
    "    return(user_coverage)\n",
    "coverage_1_train=user_coverage(recommend_movies_2_train,5,3.5)\n",
    "coverage_1_test=user_coverage(recommend_movies_2_test,5,3.5)\n",
    "print('User coverage on training set:',coverage_1_train)\n",
    "print('User coverage on test set:',coverage_1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The catalogue_coverage method calculates the movies coverage from our model. This is the proportion of movies covered in the recommendation for all the users. In other words, the fraction of items that are in the top-k for at least 1 user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalogue coverage on training set: 53.12\n",
      "Catalogue coverage on test set: 53.12\n"
     ]
    }
   ],
   "source": [
    "#Catalogue Coverage\n",
    "def catalogue_coverage(predicted, catalog):\n",
    "    predicted_flattened = [p for sublist in predicted for p in sublist]\n",
    "    unique_predictions = len(set(predicted_flattened))\n",
    "    prediction_coverage = round(unique_predictions/(len(catalog)* 1.0)*100,2)\n",
    "    return prediction_coverage\n",
    "coverage_2_train=catalogue_coverage(list(recommend_movies_2_train['recommended_movies']),list(subset_data_2['business_id_int'].unique()))\n",
    "coverage_2_test=catalogue_coverage(list(recommend_movies_2_test['recommended_movies']),list(subset_data_2['business_id_int'].unique()))\n",
    "print('Catalogue coverage on training set:',coverage_2_train)\n",
    "print('Catalogue coverage on test set:',coverage_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts=subset_data_2['user_id_int'].value_counts()\n",
    "less_prolific_users = user_counts.loc[user_counts <= 5].index.tolist()\n",
    "test_data_less_prolific=test_dataset[(test_dataset.user_id_int.isin(less_prolific_users))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for less prolific users = 1.8902777956386978\n",
      "Mean-Absolute error for less prolific users = 1.6142431840676013\n"
     ]
    }
   ],
   "source": [
    "test_less_prolific=spark.createDataFrame(test_data_less_prolific[['user_id_int','business_id_int','rating']])\n",
    "predictions_test = model.transform(test_less_prolific)\n",
    "predictions_test_pandas=predictions_test.toPandas()\n",
    "predictions_test_pandas['prediction']=np.where(predictions_test_pandas['prediction']>5,5,predictions_test_pandas['prediction'])\n",
    "predictions_test=spark.createDataFrame(predictions_test_pandas)\n",
    "rmse = accuracy(predictions_test,\"rmse\")\n",
    "print(\"Root-mean-square error for less prolific users = \" + str(rmse))\n",
    "mae = accuracy(predictions_test,\"mae\")\n",
    "print(\"Mean-Absolute error for less prolific users = \" + str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_counts=subset_data_2['business_id_int'].value_counts()\n",
    "less_popular_business = business_counts.loc[business_counts <= 100].index.tolist()\n",
    "test_data_less_popular=test_dataset[(test_dataset.business_id_int.isin(less_popular_business))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for less popular business = 1.6700578022970134\n",
      "Mean-Absolute error for less popular business = 1.3805810743454285\n"
     ]
    }
   ],
   "source": [
    "test_less_popular=spark.createDataFrame(test_data_less_popular[['user_id_int','business_id_int','rating']])\n",
    "predictions_test = model.transform(test_less_popular)\n",
    "predictions_test_pandas=predictions_test.toPandas()\n",
    "predictions_test_pandas['prediction']=np.where(predictions_test_pandas['prediction']>5,5,predictions_test_pandas['prediction'])\n",
    "predictions_test=spark.createDataFrame(predictions_test_pandas)\n",
    "rmse = accuracy(predictions_test,\"rmse\")\n",
    "print(\"Root-mean-square error for less popular business = \" + str(rmse))\n",
    "mae = accuracy(predictions_test,\"mae\")\n",
    "print(\"Mean-Absolute error for less popular business = \" + str(mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
